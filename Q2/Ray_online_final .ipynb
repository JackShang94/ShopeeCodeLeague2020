{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp /data/train.zip ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp /data/test.zip ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp /data/pretrained_resnext.pt ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp /data/test.csv ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "\n",
    "from PIL import Image # view image\n",
    "import time # \n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using =>  cuda\n",
      "The data lies here =>  \n",
      "The best model lies here =>  model/\n",
      "Lastest model checkpoint lies here =>  checkpoint/\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"We're using => \", device)\n",
    "\n",
    "root_dir = \"\"\n",
    "print(\"The data lies here => \", root_dir)\n",
    "if not os.path.exists(root_dir+'train/'):\n",
    "    os.makedirs(root_dir+'train/')\n",
    "if not os.path.exists(root_dir+'test/'):\n",
    "    os.makedirs(root_dir+'/test')\n",
    "    \n",
    "model_dir = root_dir + 'model/'\n",
    "print(\"The best model lies here => \", model_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "ckp_dir = root_dir + 'checkpoint/'\n",
    "print(\"Lastest model checkpoint lies here => \", ckp_dir)\n",
    "if not os.path.exists(ckp_dir):\n",
    "    os.makedirs(ckp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Prepare dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(25),\n",
    "#         transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 105392\n",
       "    Root location: train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "               ColorJitter(brightness=None, contrast=None, saturation=None, hue=None)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomRotation(degrees=(-25, 25), resample=False, expand=False)\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Datasets\n",
    "# Train data floders with be divied into train + val\n",
    "product_dataset = datasets.ImageFolder(root=root_dir+\"train\",\n",
    "                                      transform=image_transforms[\"train\"])\n",
    "class_names = product_dataset.classes\n",
    "product_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f58c40ff898>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_size = 0.02 # 2K\n",
    "demo_len = int(demo_size*len(product_dataset))\n",
    "tiny_dataset, _ = random_split(product_dataset, [demo_len, len(product_dataset)-demo_len])\n",
    "tiny_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're using normal big dataset\n"
     ]
    }
   ],
   "source": [
    "demo = False\n",
    "if demo == True:\n",
    "    print(\"we're using demo small dataset.\")\n",
    "    input_dataset = tiny_dataset\n",
    "else:\n",
    "    print(\"we're using normal big dataset\")\n",
    "    input_dataset = product_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation samples\n",
    "# SubsetRandomSampler\n",
    "input_dataset_size = len(input_dataset)\n",
    "input_dataset_indices = list(range(input_dataset_size))\n",
    "\n",
    "np.random.shuffle(input_dataset_indices)\n",
    "val_split_index = int(np.floor(0.2 * input_dataset_size))\n",
    "\n",
    "train_idx, val_idx = input_dataset_indices[val_split_index:],\\\n",
    "input_dataset_indices[:val_split_index]\n",
    "\n",
    "# train, val samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "# train, val dataloaders\n",
    "batch_size = 16\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(input_dataset, shuffle=False, \n",
    "                           batch_size=batch_size, sampler=train_sampler),\n",
    "    \"val\": DataLoader(input_dataset, shuffle=False, \n",
    "                        batch_size=8, sampler=val_sampler)\n",
    "}\n",
    "dataset_sizes = {'train': input_dataset_size-val_split_index, \n",
    "                 'val': val_split_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 84314, 'val': 21078}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul  4 03:48:31 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\r\n",
      "| N/A   42C    P0    61W / 280W |     12MiB / 16160MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Data Augmentation\n",
    "1. mixcut\n",
    "2. data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Load pre-trained model & model save\n",
    "1. EfficentNet trained on Imagenet. pytorch efficientnet\n",
    "2. Freeze weight of previous layers\n",
    "3. Change output layers from 1000(ImageNet) to 42 \n",
    "4. save trained models to Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torchvision.models.resnet.ResNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torchvision.models.resnet.Bottleneck' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classifier is on the device =>  cuda:0\n"
     ]
    }
   ],
   "source": [
    "#using efficientnet model based transfer learning\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # freeze trained model\n",
    "        self.model_ft = torch.load('pretrained_resnext.pt')\n",
    "        # for param in self.pretrained.parameters():\n",
    "        #   param.requires_grad = False\n",
    "        self.l1 = nn.Linear(1000 , 256)\n",
    "        self.dropout = nn.Dropout(0.75)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(256, 42)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.model_ft(input)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.dropout(self.relu(self.l1(x)))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classifier = Classifier().to(device)\n",
    "\n",
    "# check our model is on right device\n",
    "print(\"Our classifier is on the device => \", list(classifier.parameters())[0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "\n",
    "def load_ckp(checkpoint_fpath, model):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # initialize valid_acc_best from checkpoint to valid_acc_best\n",
    "    valid_acc_best = checkpoint['valid_acc_best']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, checkpoint['epoch'], valid_loss_min, valid_acc_best.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, epoch, _, best_acc = load_ckp('best_model_10am.pt', classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Loss function, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul  4 03:48:36 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    61W / 280W |   1519MiB / 16160MiB |     20%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: Train\n",
    "1.Hyperparameter  \n",
    "2.Use ResNext101 for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, checkpoint_path=None, best_model_path=None):\n",
    "    since = time.time()\n",
    "#     if os.path.exists('best/current_checkpoint.pt'):\n",
    "#         model, epoch, _, best_acc = load_ckp('checkpoint/current_checkpoint.pt', model, optimizer)\n",
    "#     else:\n",
    "#         best_acc = 0.0\n",
    "# #     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        torch.cuda.empty_cache()\n",
    "        !nvidia-smi\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            CNT = 0\n",
    "            # Iterate over data.\n",
    "            for batch_idx, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                # track history if only in train\n",
    "                # with torch.set_grad_enabled(phase == 'train'):\n",
    "                #     outputs = model(inputs)\n",
    "                #     _, preds = torch.max(outputs, 1)\n",
    "                #     loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                batchsize = inputs.size(0)\n",
    "                running_loss += loss.item() * batchsize\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                CNT += 1\n",
    "                if batch_idx % 100 == 0:\n",
    "                    print('   {}:  [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tAcc: {:.6f}'.format(phase,\n",
    "                        CNT*batchsize, dataset_sizes[phase], 100. * CNT/len(dataloaders[phase]),\n",
    "                        running_loss/(CNT*batchsize), running_corrects.double()/(CNT*batchsize)))\n",
    "#                 if batch_idx == 300:\n",
    "#                     break\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            # create checkpoint variable and add important data\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'valid_loss_min': epoch_loss,\n",
    "                'valid_acc_best': epoch_acc,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "                }            \n",
    "            # deep copy and save the model\n",
    "            if phase == 'val':\n",
    "                if epoch_acc >= best_acc:\n",
    "                    save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "                    best_acc = epoch_acc\n",
    "#                     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                else:\n",
    "                    save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "            print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # return the best model weights\n",
    "    model, _, _, _ = load_ckp('model/best_model.pt', model, optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "Sat Jul  4 03:48:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    61W / 280W |   1519MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train:  [16/84314 (0%)]\tLoss: 0.543112 \tAcc: 0.812500\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.736437 \tAcc: 0.798886\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.704434 \tAcc: 0.814988\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.681513 \tAcc: 0.821221\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.670609 \tAcc: 0.822007\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.660338 \tAcc: 0.825100\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.655508 \tAcc: 0.825603\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.656891 \tAcc: 0.826854\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.645355 \tAcc: 0.829744\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.633824 \tAcc: 0.831853\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.636788 \tAcc: 0.831231\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.631507 \tAcc: 0.832312\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.628688 \tAcc: 0.832379\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.625442 \tAcc: 0.833061\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.625628 \tAcc: 0.833378\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.628590 \tAcc: 0.833611\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.627839 \tAcc: 0.833581\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.628956 \tAcc: 0.833113\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.625187 \tAcc: 0.833842\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.623940 \tAcc: 0.833969\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.619819 \tAcc: 0.834333\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.617385 \tAcc: 0.834513\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.615744 \tAcc: 0.834677\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.612651 \tAcc: 0.835561\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.610457 \tAcc: 0.836006\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.609377 \tAcc: 0.836116\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.610640 \tAcc: 0.835880\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.608334 \tAcc: 0.836380\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.607688 \tAcc: 0.836532\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.606924 \tAcc: 0.837060\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.603737 \tAcc: 0.837908\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.601985 \tAcc: 0.838379\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.601511 \tAcc: 0.838566\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.600993 \tAcc: 0.838647\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.600638 \tAcc: 0.838687\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.599356 \tAcc: 0.839403\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.598537 \tAcc: 0.839333\n",
      "   train:  [59216/84314 (70%)]\tLoss: 0.597996 \tAcc: 0.839317\n",
      "   train:  [60816/84314 (72%)]\tLoss: 0.595928 \tAcc: 0.839828\n",
      "   train:  [62416/84314 (74%)]\tLoss: 0.594229 \tAcc: 0.840137\n",
      "   train:  [64016/84314 (76%)]\tLoss: 0.594225 \tAcc: 0.840446\n",
      "   train:  [65616/84314 (78%)]\tLoss: 0.592061 \tAcc: 0.841030\n",
      "   train:  [67216/84314 (80%)]\tLoss: 0.592291 \tAcc: 0.840931\n",
      "   train:  [68816/84314 (82%)]\tLoss: 0.593301 \tAcc: 0.840706\n",
      "   train:  [70416/84314 (84%)]\tLoss: 0.592260 \tAcc: 0.841059\n",
      "   train:  [72016/84314 (85%)]\tLoss: 0.591910 \tAcc: 0.841021\n",
      "   train:  [73616/84314 (87%)]\tLoss: 0.591028 \tAcc: 0.841339\n",
      "   train:  [75216/84314 (89%)]\tLoss: 0.589359 \tAcc: 0.841723\n",
      "   train:  [76816/84314 (91%)]\tLoss: 0.588899 \tAcc: 0.841973\n",
      "   train:  [78416/84314 (93%)]\tLoss: 0.587950 \tAcc: 0.842162\n",
      "   train:  [80016/84314 (95%)]\tLoss: 0.587568 \tAcc: 0.842294\n",
      "   train:  [81616/84314 (97%)]\tLoss: 0.587518 \tAcc: 0.842249\n",
      "   train:  [83216/84314 (99%)]\tLoss: 0.586893 \tAcc: 0.842398\n",
      "train Loss: 0.5868 Acc: 0.8424\n",
      "\n",
      "   val:  [8/21078 (0%)]\tLoss: 0.276152 \tAcc: 1.000000\n",
      "   val:  [808/21078 (4%)]\tLoss: 0.422296 \tAcc: 0.873762\n",
      "   val:  [1608/21078 (8%)]\tLoss: 0.460874 \tAcc: 0.860075\n",
      "   val:  [2408/21078 (11%)]\tLoss: 0.423549 \tAcc: 0.877492\n",
      "   val:  [3208/21078 (15%)]\tLoss: 0.417830 \tAcc: 0.878741\n",
      "   val:  [4008/21078 (19%)]\tLoss: 0.415415 \tAcc: 0.879990\n",
      "   val:  [4808/21078 (23%)]\tLoss: 0.422984 \tAcc: 0.878120\n",
      "   val:  [5608/21078 (27%)]\tLoss: 0.421584 \tAcc: 0.879458\n",
      "   val:  [6408/21078 (30%)]\tLoss: 0.418550 \tAcc: 0.878589\n",
      "   val:  [7208/21078 (34%)]\tLoss: 0.420336 \tAcc: 0.878330\n",
      "   val:  [8008/21078 (38%)]\tLoss: 0.423194 \tAcc: 0.878372\n",
      "   val:  [8808/21078 (42%)]\tLoss: 0.427487 \tAcc: 0.877498\n",
      "   val:  [9608/21078 (46%)]\tLoss: 0.429325 \tAcc: 0.877186\n",
      "   val:  [10408/21078 (49%)]\tLoss: 0.425525 \tAcc: 0.878555\n",
      "   val:  [11208/21078 (53%)]\tLoss: 0.426751 \tAcc: 0.878926\n",
      "   val:  [12008/21078 (57%)]\tLoss: 0.429519 \tAcc: 0.879164\n",
      "   val:  [12808/21078 (61%)]\tLoss: 0.431020 \tAcc: 0.878826\n",
      "   val:  [13608/21078 (65%)]\tLoss: 0.432297 \tAcc: 0.878307\n",
      "   val:  [14408/21078 (68%)]\tLoss: 0.430040 \tAcc: 0.878262\n",
      "   val:  [15208/21078 (72%)]\tLoss: 0.429849 \tAcc: 0.878353\n",
      "   val:  [16008/21078 (76%)]\tLoss: 0.429536 \tAcc: 0.878186\n",
      "   val:  [16808/21078 (80%)]\tLoss: 0.431885 \tAcc: 0.877499\n",
      "   val:  [17608/21078 (84%)]\tLoss: 0.430169 \tAcc: 0.877556\n",
      "   val:  [18408/21078 (87%)]\tLoss: 0.430934 \tAcc: 0.877390\n",
      "   val:  [19208/21078 (91%)]\tLoss: 0.433390 \tAcc: 0.876822\n",
      "   val:  [20008/21078 (95%)]\tLoss: 0.430357 \tAcc: 0.877249\n",
      "   val:  [20808/21078 (99%)]\tLoss: 0.432876 \tAcc: 0.876682\n",
      "val Loss: 0.4319 Acc: 0.8769\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "Sat Jul  4 04:33:10 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    58W / 280W |   5212MiB / 16160MiB |     42%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "   train:  [16/84314 (0%)]\tLoss: 0.226896 \tAcc: 1.000000\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.555391 \tAcc: 0.851485\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.540265 \tAcc: 0.856654\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.538553 \tAcc: 0.859012\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.532656 \tAcc: 0.859102\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.529012 \tAcc: 0.858907\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.522497 \tAcc: 0.860545\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.524761 \tAcc: 0.859130\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.526725 \tAcc: 0.858848\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.524136 \tAcc: 0.859947\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.525348 \tAcc: 0.859453\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.521427 \tAcc: 0.860070\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.525367 \tAcc: 0.859024\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.523496 \tAcc: 0.859339\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.524302 \tAcc: 0.858539\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.524781 \tAcc: 0.858469\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.522286 \tAcc: 0.859151\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.526641 \tAcc: 0.858539\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.526441 \tAcc: 0.858690\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.524217 \tAcc: 0.858956\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.523801 \tAcc: 0.858914\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.521492 \tAcc: 0.859531\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.520576 \tAcc: 0.859638\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.522550 \tAcc: 0.859083\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.523297 \tAcc: 0.858861\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.523159 \tAcc: 0.858532\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.521723 \tAcc: 0.858732\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.522108 \tAcc: 0.858409\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.521401 \tAcc: 0.858510\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.519907 \tAcc: 0.859143\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.518677 \tAcc: 0.859380\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.518550 \tAcc: 0.859340\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.516409 \tAcc: 0.859790\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.516627 \tAcc: 0.859910\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.514987 \tAcc: 0.860354\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.513879 \tAcc: 0.860451\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.512601 \tAcc: 0.860872\n",
      "   train:  [59216/84314 (70%)]\tLoss: 0.510845 \tAcc: 0.861237\n",
      "   train:  [60816/84314 (72%)]\tLoss: 0.510273 \tAcc: 0.861566\n",
      "   train:  [62416/84314 (74%)]\tLoss: 0.509479 \tAcc: 0.861734\n",
      "   train:  [64016/84314 (76%)]\tLoss: 0.511294 \tAcc: 0.861457\n",
      "   train:  [65616/84314 (78%)]\tLoss: 0.512920 \tAcc: 0.861101\n",
      "   train:  [67216/84314 (80%)]\tLoss: 0.513819 \tAcc: 0.860881\n",
      "   train:  [68816/84314 (82%)]\tLoss: 0.515472 \tAcc: 0.860527\n",
      "   train:  [70416/84314 (84%)]\tLoss: 0.515267 \tAcc: 0.860642\n",
      "   train:  [72016/84314 (85%)]\tLoss: 0.515453 \tAcc: 0.860670\n",
      "   train:  [73616/84314 (87%)]\tLoss: 0.515295 \tAcc: 0.860818\n",
      "   train:  [75216/84314 (89%)]\tLoss: 0.514968 \tAcc: 0.860841\n",
      "   train:  [76816/84314 (91%)]\tLoss: 0.515717 \tAcc: 0.860862\n",
      "   train:  [78416/84314 (93%)]\tLoss: 0.515987 \tAcc: 0.860781\n",
      "   train:  [80016/84314 (95%)]\tLoss: 0.516882 \tAcc: 0.860465\n",
      "   train:  [81616/84314 (97%)]\tLoss: 0.516527 \tAcc: 0.860420\n",
      "   train:  [83216/84314 (99%)]\tLoss: 0.516482 \tAcc: 0.860303\n",
      "train Loss: 0.5163 Acc: 0.8604\n",
      "\n",
      "   val:  [8/21078 (0%)]\tLoss: 0.569721 \tAcc: 0.750000\n",
      "   val:  [808/21078 (4%)]\tLoss: 0.486545 \tAcc: 0.860149\n",
      "   val:  [1608/21078 (8%)]\tLoss: 0.437057 \tAcc: 0.874378\n",
      "   val:  [2408/21078 (11%)]\tLoss: 0.463679 \tAcc: 0.866279\n",
      "   val:  [3208/21078 (15%)]\tLoss: 0.467300 \tAcc: 0.866584\n",
      "   val:  [4008/21078 (19%)]\tLoss: 0.442969 \tAcc: 0.870758\n",
      "   val:  [4808/21078 (23%)]\tLoss: 0.441754 \tAcc: 0.871256\n",
      "   val:  [5608/21078 (27%)]\tLoss: 0.434956 \tAcc: 0.872325\n",
      "   val:  [6408/21078 (30%)]\tLoss: 0.441334 \tAcc: 0.870162\n",
      "   val:  [7208/21078 (34%)]\tLoss: 0.448350 \tAcc: 0.869451\n",
      "   val:  [8008/21078 (38%)]\tLoss: 0.446925 \tAcc: 0.870005\n",
      "   val:  [8808/21078 (42%)]\tLoss: 0.447020 \tAcc: 0.869437\n",
      "   val:  [9608/21078 (46%)]\tLoss: 0.448677 \tAcc: 0.868339\n",
      "   val:  [10408/21078 (49%)]\tLoss: 0.448618 \tAcc: 0.868947\n",
      "   val:  [11208/21078 (53%)]\tLoss: 0.447762 \tAcc: 0.869647\n",
      "   val:  [12008/21078 (57%)]\tLoss: 0.444567 \tAcc: 0.871086\n",
      "   val:  [12808/21078 (61%)]\tLoss: 0.445086 \tAcc: 0.871252\n",
      "   val:  [13608/21078 (65%)]\tLoss: 0.444384 \tAcc: 0.871032\n",
      "   val:  [14408/21078 (68%)]\tLoss: 0.441772 \tAcc: 0.871460\n",
      "   val:  [15208/21078 (72%)]\tLoss: 0.441767 \tAcc: 0.870529\n",
      "   val:  [16008/21078 (76%)]\tLoss: 0.442276 \tAcc: 0.870752\n",
      "   val:  [16808/21078 (80%)]\tLoss: 0.442626 \tAcc: 0.870359\n",
      "   val:  [17608/21078 (84%)]\tLoss: 0.443561 \tAcc: 0.870116\n",
      "   val:  [18408/21078 (87%)]\tLoss: 0.442546 \tAcc: 0.870165\n",
      "   val:  [19208/21078 (91%)]\tLoss: 0.441611 \tAcc: 0.870367\n",
      "   val:  [20008/21078 (95%)]\tLoss: 0.442092 \tAcc: 0.869802\n",
      "   val:  [20808/21078 (99%)]\tLoss: 0.441335 \tAcc: 0.869954\n",
      "val Loss: 0.4411 Acc: 0.8702\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "Sat Jul  4 05:24:55 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    64W / 280W |   6680MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "   train:  [16/84314 (0%)]\tLoss: 0.177901 \tAcc: 1.000000\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.445202 \tAcc: 0.871287\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.508895 \tAcc: 0.858831\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.498857 \tAcc: 0.861503\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.482752 \tAcc: 0.865181\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.480818 \tAcc: 0.864770\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.473606 \tAcc: 0.866473\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.471837 \tAcc: 0.868046\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.474158 \tAcc: 0.867900\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.474826 \tAcc: 0.868341\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.470398 \tAcc: 0.869256\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.468694 \tAcc: 0.869834\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.465941 \tAcc: 0.870941\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.468103 \tAcc: 0.870628\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.470745 \tAcc: 0.870227\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.473198 \tAcc: 0.869046\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.474758 \tAcc: 0.868832\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.473790 \tAcc: 0.869268\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.474598 \tAcc: 0.869309\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.478071 \tAcc: 0.868359\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.480177 \tAcc: 0.867847\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.479208 \tAcc: 0.867771\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.478435 \tAcc: 0.867873\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.476782 \tAcc: 0.868237\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.477045 \tAcc: 0.867894\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.475650 \tAcc: 0.868578\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.476332 \tAcc: 0.868752\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.477433 \tAcc: 0.868729\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.475862 \tAcc: 0.868975\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.475840 \tAcc: 0.868817\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.474652 \tAcc: 0.869002\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.473690 \tAcc: 0.869316\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.474182 \tAcc: 0.869318\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.474313 \tAcc: 0.869093\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.472687 \tAcc: 0.869432\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.474068 \tAcc: 0.869287\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.473140 \tAcc: 0.869533\n",
      "   train:  [59216/84314 (70%)]\tLoss: 0.473387 \tAcc: 0.869410\n",
      "   train:  [60816/84314 (72%)]\tLoss: 0.473128 \tAcc: 0.869541\n",
      "   train:  [62416/84314 (74%)]\tLoss: 0.472357 \tAcc: 0.869825\n",
      "   train:  [64016/84314 (76%)]\tLoss: 0.472636 \tAcc: 0.869689\n",
      "   train:  [65616/84314 (78%)]\tLoss: 0.472828 \tAcc: 0.869605\n",
      "   train:  [67216/84314 (80%)]\tLoss: 0.473016 \tAcc: 0.869689\n",
      "   train:  [68816/84314 (82%)]\tLoss: 0.472587 \tAcc: 0.869725\n",
      "   train:  [70416/84314 (84%)]\tLoss: 0.472288 \tAcc: 0.869646\n",
      "   train:  [72016/84314 (85%)]\tLoss: 0.471603 \tAcc: 0.869765\n",
      "   train:  [73616/84314 (87%)]\tLoss: 0.471146 \tAcc: 0.869974\n",
      "   train:  [75216/84314 (89%)]\tLoss: 0.471372 \tAcc: 0.869895\n",
      "   train:  [76816/84314 (91%)]\tLoss: 0.471772 \tAcc: 0.869845\n",
      "   train:  [78416/84314 (93%)]\tLoss: 0.472758 \tAcc: 0.869351\n",
      "   train:  [80016/84314 (95%)]\tLoss: 0.472201 \tAcc: 0.869539\n",
      "   train:  [81616/84314 (97%)]\tLoss: 0.472501 \tAcc: 0.869609\n",
      "   train:  [83216/84314 (99%)]\tLoss: 0.472095 \tAcc: 0.869689\n",
      "train Loss: 0.4720 Acc: 0.8697\n",
      "\n",
      "   val:  [8/21078 (0%)]\tLoss: 0.000161 \tAcc: 1.000000\n",
      "   val:  [808/21078 (4%)]\tLoss: 0.425041 \tAcc: 0.875000\n",
      "   val:  [1608/21078 (8%)]\tLoss: 0.433456 \tAcc: 0.871891\n",
      "   val:  [2408/21078 (11%)]\tLoss: 0.419902 \tAcc: 0.880814\n",
      "   val:  [3208/21078 (15%)]\tLoss: 0.412696 \tAcc: 0.884352\n",
      "   val:  [4008/21078 (19%)]\tLoss: 0.417220 \tAcc: 0.880739\n",
      "   val:  [4808/21078 (23%)]\tLoss: 0.433040 \tAcc: 0.878328\n",
      "   val:  [5608/21078 (27%)]\tLoss: 0.440066 \tAcc: 0.877675\n",
      "   val:  [6408/21078 (30%)]\tLoss: 0.440169 \tAcc: 0.878433\n",
      "   val:  [7208/21078 (34%)]\tLoss: 0.439239 \tAcc: 0.878885\n",
      "   val:  [8008/21078 (38%)]\tLoss: 0.432681 \tAcc: 0.878996\n",
      "   val:  [8808/21078 (42%)]\tLoss: 0.435371 \tAcc: 0.877271\n",
      "   val:  [9608/21078 (46%)]\tLoss: 0.438043 \tAcc: 0.877810\n",
      "   val:  [10408/21078 (49%)]\tLoss: 0.429760 \tAcc: 0.879996\n",
      "   val:  [11208/21078 (53%)]\tLoss: 0.430586 \tAcc: 0.879907\n",
      "   val:  [12008/21078 (57%)]\tLoss: 0.433231 \tAcc: 0.878581\n",
      "   val:  [12808/21078 (61%)]\tLoss: 0.432140 \tAcc: 0.878592\n",
      "   val:  [13608/21078 (65%)]\tLoss: 0.438595 \tAcc: 0.876690\n",
      "   val:  [14408/21078 (68%)]\tLoss: 0.438104 \tAcc: 0.876527\n",
      "   val:  [15208/21078 (72%)]\tLoss: 0.440764 \tAcc: 0.875723\n",
      "   val:  [16008/21078 (76%)]\tLoss: 0.438282 \tAcc: 0.876624\n",
      "   val:  [16808/21078 (80%)]\tLoss: 0.438926 \tAcc: 0.876844\n",
      "   val:  [17608/21078 (84%)]\tLoss: 0.434264 \tAcc: 0.877953\n",
      "   val:  [18408/21078 (87%)]\tLoss: 0.432962 \tAcc: 0.878205\n",
      "   val:  [19208/21078 (91%)]\tLoss: 0.438098 \tAcc: 0.876978\n",
      "   val:  [20008/21078 (95%)]\tLoss: 0.438863 \tAcc: 0.877499\n",
      "   val:  [20808/21078 (99%)]\tLoss: 0.438675 \tAcc: 0.877115\n",
      "val Loss: 0.4396 Acc: 0.8768\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "Sat Jul  4 06:18:24 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    64W / 280W |   6946MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "   train:  [16/84314 (0%)]\tLoss: 0.152019 \tAcc: 1.000000\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.440145 \tAcc: 0.886757\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.427658 \tAcc: 0.884328\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.439770 \tAcc: 0.882683\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.446811 \tAcc: 0.882014\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.445144 \tAcc: 0.880739\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.446246 \tAcc: 0.878536\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.442700 \tAcc: 0.878121\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.441169 \tAcc: 0.878121\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.437573 \tAcc: 0.879370\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.432749 \tAcc: 0.880307\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.436033 \tAcc: 0.879428\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.433305 \tAcc: 0.880776\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.433208 \tAcc: 0.881005\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.434121 \tAcc: 0.880219\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.434669 \tAcc: 0.879788\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.435290 \tAcc: 0.879450\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.434376 \tAcc: 0.879372\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.433476 \tAcc: 0.879650\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.436709 \tAcc: 0.878617\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.435093 \tAcc: 0.878842\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.434327 \tAcc: 0.878956\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.436126 \tAcc: 0.878493\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.438377 \tAcc: 0.878259\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.441098 \tAcc: 0.877525\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.442586 \tAcc: 0.877074\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.441056 \tAcc: 0.877499\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.441997 \tAcc: 0.877314\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.441180 \tAcc: 0.877588\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.443951 \tAcc: 0.877198\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.445089 \tAcc: 0.876833\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.444483 \tAcc: 0.876975\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.444693 \tAcc: 0.876894\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.444739 \tAcc: 0.877140\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.444827 \tAcc: 0.877040\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.444940 \tAcc: 0.877160\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.442620 \tAcc: 0.877760\n",
      "   train:  [59216/84314 (70%)]\tLoss: 0.441983 \tAcc: 0.877938\n",
      "   train:  [60816/84314 (72%)]\tLoss: 0.442439 \tAcc: 0.877812\n",
      "   train:  [62416/84314 (74%)]\tLoss: 0.442004 \tAcc: 0.877868\n",
      "   train:  [64016/84314 (76%)]\tLoss: 0.442423 \tAcc: 0.877827\n",
      "   train:  [65616/84314 (78%)]\tLoss: 0.442324 \tAcc: 0.877850\n",
      "   train:  [67216/84314 (80%)]\tLoss: 0.441589 \tAcc: 0.877871\n",
      "   train:  [68816/84314 (82%)]\tLoss: 0.442022 \tAcc: 0.877776\n",
      "   train:  [70416/84314 (84%)]\tLoss: 0.440999 \tAcc: 0.878067\n",
      "   train:  [72016/84314 (85%)]\tLoss: 0.442179 \tAcc: 0.877597\n",
      "   train:  [73616/84314 (87%)]\tLoss: 0.441935 \tAcc: 0.877567\n",
      "   train:  [75216/84314 (89%)]\tLoss: 0.442295 \tAcc: 0.877420\n",
      "   train:  [76816/84314 (91%)]\tLoss: 0.443066 \tAcc: 0.877070\n",
      "   train:  [78416/84314 (93%)]\tLoss: 0.442653 \tAcc: 0.877244\n",
      "   train:  [80016/84314 (95%)]\tLoss: 0.442635 \tAcc: 0.877162\n",
      "   train:  [81616/84314 (97%)]\tLoss: 0.442085 \tAcc: 0.877193\n",
      "   train:  [83216/84314 (99%)]\tLoss: 0.441680 \tAcc: 0.877271\n",
      "train Loss: 0.4413 Acc: 0.8774\n",
      "\n",
      "   val:  [8/21078 (0%)]\tLoss: 0.979751 \tAcc: 0.750000\n",
      "   val:  [808/21078 (4%)]\tLoss: 0.454911 \tAcc: 0.873762\n",
      "   val:  [1608/21078 (8%)]\tLoss: 0.425731 \tAcc: 0.883706\n",
      "   val:  [2408/21078 (11%)]\tLoss: 0.417776 \tAcc: 0.882890\n",
      "   val:  [3208/21078 (15%)]\tLoss: 0.424699 \tAcc: 0.883728\n",
      "   val:  [4008/21078 (19%)]\tLoss: 0.422539 \tAcc: 0.882485\n",
      "   val:  [4808/21078 (23%)]\tLoss: 0.433571 \tAcc: 0.878744\n",
      "   val:  [5608/21078 (27%)]\tLoss: 0.430378 \tAcc: 0.878210\n",
      "   val:  [6408/21078 (30%)]\tLoss: 0.437480 \tAcc: 0.877497\n",
      "   val:  [7208/21078 (34%)]\tLoss: 0.439819 \tAcc: 0.877358\n",
      "   val:  [8008/21078 (38%)]\tLoss: 0.442726 \tAcc: 0.875250\n",
      "   val:  [8808/21078 (42%)]\tLoss: 0.446955 \tAcc: 0.875341\n",
      "   val:  [9608/21078 (46%)]\tLoss: 0.446598 \tAcc: 0.875312\n",
      "   val:  [10408/21078 (49%)]\tLoss: 0.445306 \tAcc: 0.875961\n",
      "   val:  [11208/21078 (53%)]\tLoss: 0.439572 \tAcc: 0.877944\n",
      "   val:  [12008/21078 (57%)]\tLoss: 0.441222 \tAcc: 0.877498\n",
      "   val:  [12808/21078 (61%)]\tLoss: 0.442814 \tAcc: 0.876874\n",
      "   val:  [13608/21078 (65%)]\tLoss: 0.443748 \tAcc: 0.876690\n",
      "   val:  [14408/21078 (68%)]\tLoss: 0.442642 \tAcc: 0.877499\n",
      "   val:  [15208/21078 (72%)]\tLoss: 0.445935 \tAcc: 0.876578\n",
      "   val:  [16008/21078 (76%)]\tLoss: 0.444856 \tAcc: 0.876999\n",
      "   val:  [16808/21078 (80%)]\tLoss: 0.446608 \tAcc: 0.876368\n",
      "   val:  [17608/21078 (84%)]\tLoss: 0.448780 \tAcc: 0.875738\n",
      "   val:  [18408/21078 (87%)]\tLoss: 0.449851 \tAcc: 0.875109\n",
      "   val:  [19208/21078 (91%)]\tLoss: 0.448851 \tAcc: 0.875208\n",
      "   val:  [20008/21078 (95%)]\tLoss: 0.447589 \tAcc: 0.875350\n",
      "   val:  [20808/21078 (99%)]\tLoss: 0.447651 \tAcc: 0.874904\n",
      "val Loss: 0.4472 Acc: 0.8752\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "Sat Jul  4 07:21:44 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    64W / 280W |   6818MiB / 16160MiB |     32%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "   train:  [16/84314 (0%)]\tLoss: 0.222447 \tAcc: 0.875000\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.456004 \tAcc: 0.883663\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.437754 \tAcc: 0.890236\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.426062 \tAcc: 0.888497\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.416963 \tAcc: 0.890586\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.411987 \tAcc: 0.890719\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.398842 \tAcc: 0.892159\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.394509 \tAcc: 0.893277\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.387686 \tAcc: 0.894117\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.386327 \tAcc: 0.894076\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.387452 \tAcc: 0.893856\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.385880 \tAcc: 0.893052\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.387185 \tAcc: 0.892798\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.385758 \tAcc: 0.892679\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.387773 \tAcc: 0.892264\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.387745 \tAcc: 0.891864\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.388637 \tAcc: 0.891630\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.388183 \tAcc: 0.891314\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.390523 \tAcc: 0.890686\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.390662 \tAcc: 0.890946\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.390480 \tAcc: 0.891086\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.392923 \tAcc: 0.890142\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.392602 \tAcc: 0.889851\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.394074 \tAcc: 0.889559\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.395253 \tAcc: 0.889213\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.394327 \tAcc: 0.889694\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.393387 \tAcc: 0.890114\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.392136 \tAcc: 0.890133\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.392473 \tAcc: 0.890195\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.392905 \tAcc: 0.890059\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.392541 \tAcc: 0.890141\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.392333 \tAcc: 0.890096\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.393443 \tAcc: 0.889624\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.393455 \tAcc: 0.889371\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.393986 \tAcc: 0.889113\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.392761 \tAcc: 0.889157\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.392207 \tAcc: 0.889579\n",
      "   train:  [59216/84314 (70%)]\tLoss: 0.391193 \tAcc: 0.889743\n",
      "   train:  [60816/84314 (72%)]\tLoss: 0.390971 \tAcc: 0.889799\n",
      "   train:  [62416/84314 (74%)]\tLoss: 0.391682 \tAcc: 0.889644\n",
      "   train:  [64016/84314 (76%)]\tLoss: 0.390827 \tAcc: 0.889965\n",
      "   train:  [65616/84314 (78%)]\tLoss: 0.390748 \tAcc: 0.890088\n",
      "   train:  [67216/84314 (80%)]\tLoss: 0.390184 \tAcc: 0.890383\n",
      "   train:  [68816/84314 (82%)]\tLoss: 0.389335 \tAcc: 0.890650\n",
      "   train:  [70416/84314 (84%)]\tLoss: 0.387997 \tAcc: 0.891047\n",
      "   train:  [72016/84314 (85%)]\tLoss: 0.387571 \tAcc: 0.891121\n",
      "   train:  [73616/84314 (87%)]\tLoss: 0.387938 \tAcc: 0.891002\n",
      "   train:  [75216/84314 (89%)]\tLoss: 0.387551 \tAcc: 0.891074\n",
      "   train:  [76816/84314 (91%)]\tLoss: 0.388028 \tAcc: 0.891012\n",
      "   train:  [78416/84314 (93%)]\tLoss: 0.388039 \tAcc: 0.891094\n",
      "   train:  [80016/84314 (95%)]\tLoss: 0.387265 \tAcc: 0.891234\n",
      "   train:  [81616/84314 (97%)]\tLoss: 0.387100 \tAcc: 0.891333\n",
      "   train:  [83216/84314 (99%)]\tLoss: 0.386898 \tAcc: 0.891415\n",
      "train Loss: 0.3870 Acc: 0.8914\n",
      "\n",
      "   val:  [8/21078 (0%)]\tLoss: 0.069698 \tAcc: 1.000000\n",
      "   val:  [808/21078 (4%)]\tLoss: 0.405260 \tAcc: 0.882426\n",
      "   val:  [1608/21078 (8%)]\tLoss: 0.451997 \tAcc: 0.879975\n",
      "   val:  [2408/21078 (11%)]\tLoss: 0.419948 \tAcc: 0.887874\n",
      "   val:  [3208/21078 (15%)]\tLoss: 0.425760 \tAcc: 0.886222\n",
      "   val:  [4008/21078 (19%)]\tLoss: 0.436432 \tAcc: 0.881986\n",
      "   val:  [4808/21078 (23%)]\tLoss: 0.439050 \tAcc: 0.882696\n",
      "   val:  [5608/21078 (27%)]\tLoss: 0.440883 \tAcc: 0.881419\n",
      "   val:  [6408/21078 (30%)]\tLoss: 0.453653 \tAcc: 0.877341\n",
      "   val:  [7208/21078 (34%)]\tLoss: 0.456334 \tAcc: 0.877636\n",
      "   val:  [8008/21078 (38%)]\tLoss: 0.451966 \tAcc: 0.877498\n",
      "   val:  [8808/21078 (42%)]\tLoss: 0.456424 \tAcc: 0.876817\n",
      "   val:  [9608/21078 (46%)]\tLoss: 0.455861 \tAcc: 0.876457\n",
      "   val:  [10408/21078 (49%)]\tLoss: 0.452481 \tAcc: 0.876633\n",
      "   val:  [11208/21078 (53%)]\tLoss: 0.449685 \tAcc: 0.876874\n",
      "   val:  [12008/21078 (57%)]\tLoss: 0.443999 \tAcc: 0.878081\n",
      "   val:  [12808/21078 (61%)]\tLoss: 0.443400 \tAcc: 0.877889\n",
      "   val:  [13608/21078 (65%)]\tLoss: 0.440539 \tAcc: 0.878674\n",
      "   val:  [14408/21078 (68%)]\tLoss: 0.438281 \tAcc: 0.878679\n",
      "   val:  [15208/21078 (72%)]\tLoss: 0.434595 \tAcc: 0.878814\n",
      "   val:  [16008/21078 (76%)]\tLoss: 0.435978 \tAcc: 0.877561\n",
      "   val:  [16808/21078 (80%)]\tLoss: 0.436946 \tAcc: 0.876904\n",
      "   val:  [17608/21078 (84%)]\tLoss: 0.436927 \tAcc: 0.877272\n",
      "   val:  [18408/21078 (87%)]\tLoss: 0.437406 \tAcc: 0.877390\n",
      "   val:  [19208/21078 (91%)]\tLoss: 0.434126 \tAcc: 0.878748\n",
      "   val:  [20008/21078 (95%)]\tLoss: 0.435362 \tAcc: 0.878249\n",
      "   val:  [20808/21078 (99%)]\tLoss: 0.432898 \tAcc: 0.878797\n",
      "val Loss: 0.4341 Acc: 0.8783\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "Sat Jul  4 08:07:33 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    65W / 280W |   7112MiB / 16160MiB |     40%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "   train:  [16/84314 (0%)]\tLoss: 0.738054 \tAcc: 0.812500\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.354160 \tAcc: 0.904084\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.365767 \tAcc: 0.902674\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.361912 \tAcc: 0.900955\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.367327 \tAcc: 0.899002\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.363884 \tAcc: 0.898827\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.360686 \tAcc: 0.899126\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.361961 \tAcc: 0.898805\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.362545 \tAcc: 0.898486\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.358175 \tAcc: 0.898654\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.358478 \tAcc: 0.898851\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.359978 \tAcc: 0.898955\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.360332 \tAcc: 0.899095\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.360506 \tAcc: 0.898972\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.360312 \tAcc: 0.899491\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.361454 \tAcc: 0.899817\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.361652 \tAcc: 0.899633\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.361708 \tAcc: 0.899618\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.361789 \tAcc: 0.899778\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.363457 \tAcc: 0.899428\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.365856 \tAcc: 0.898801\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.365891 \tAcc: 0.898590\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.364206 \tAcc: 0.898938\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.366923 \tAcc: 0.898441\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.367466 \tAcc: 0.898480\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.366936 \tAcc: 0.898441\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.365274 \tAcc: 0.898525\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.365702 \tAcc: 0.898255\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.363299 \tAcc: 0.898518\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.363330 \tAcc: 0.898483\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.363336 \tAcc: 0.898638\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.362953 \tAcc: 0.898621\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.364120 \tAcc: 0.898528\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.364548 \tAcc: 0.898572\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.366403 \tAcc: 0.898008\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.366583 \tAcc: 0.897940\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.367218 \tAcc: 0.897737\n",
      "   train:  [59216/84314 (70%)]\tLoss: 0.367814 \tAcc: 0.897578\n",
      "   train:  [60816/84314 (72%)]\tLoss: 0.369230 \tAcc: 0.897313\n",
      "   train:  [62416/84314 (74%)]\tLoss: 0.369371 \tAcc: 0.897190\n",
      "   train:  [64016/84314 (76%)]\tLoss: 0.368597 \tAcc: 0.897385\n",
      "   train:  [65616/84314 (78%)]\tLoss: 0.368739 \tAcc: 0.897342\n",
      "   train:  [67216/84314 (80%)]\tLoss: 0.368734 \tAcc: 0.897391\n",
      "   train:  [68816/84314 (82%)]\tLoss: 0.367983 \tAcc: 0.897655\n",
      "   train:  [70416/84314 (84%)]\tLoss: 0.367515 \tAcc: 0.897694\n",
      "   train:  [72016/84314 (85%)]\tLoss: 0.367647 \tAcc: 0.897703\n",
      "   train:  [73616/84314 (87%)]\tLoss: 0.367854 \tAcc: 0.897577\n",
      "   train:  [75216/84314 (89%)]\tLoss: 0.368767 \tAcc: 0.897376\n",
      "   train:  [76816/84314 (91%)]\tLoss: 0.368293 \tAcc: 0.897443\n",
      "   train:  [78416/84314 (93%)]\tLoss: 0.368763 \tAcc: 0.897164\n",
      "   train:  [80016/84314 (95%)]\tLoss: 0.367877 \tAcc: 0.897408\n",
      "   train:  [81616/84314 (97%)]\tLoss: 0.367517 \tAcc: 0.897557\n",
      "   train:  [83216/84314 (99%)]\tLoss: 0.367791 \tAcc: 0.897616\n",
      "train Loss: 0.3680 Acc: 0.8975\n",
      "\n",
      "   val:  [8/21078 (0%)]\tLoss: 0.826004 \tAcc: 0.875000\n",
      "   val:  [808/21078 (4%)]\tLoss: 0.432969 \tAcc: 0.879950\n",
      "   val:  [1608/21078 (8%)]\tLoss: 0.445695 \tAcc: 0.876244\n",
      "   val:  [2408/21078 (11%)]\tLoss: 0.433097 \tAcc: 0.879983\n",
      "   val:  [3208/21078 (15%)]\tLoss: 0.427449 \tAcc: 0.880299\n",
      "   val:  [4008/21078 (19%)]\tLoss: 0.436405 \tAcc: 0.876497\n",
      "   val:  [4808/21078 (23%)]\tLoss: 0.444275 \tAcc: 0.874792\n",
      "   val:  [5608/21078 (27%)]\tLoss: 0.435246 \tAcc: 0.878923\n",
      "   val:  [6408/21078 (30%)]\tLoss: 0.432479 \tAcc: 0.879682\n",
      "   val:  [7208/21078 (34%)]\tLoss: 0.434660 \tAcc: 0.879162\n",
      "   val:  [8008/21078 (38%)]\tLoss: 0.434994 \tAcc: 0.880120\n",
      "   val:  [8808/21078 (42%)]\tLoss: 0.435230 \tAcc: 0.880336\n",
      "   val:  [9608/21078 (46%)]\tLoss: 0.437361 \tAcc: 0.879580\n",
      "   val:  [10408/21078 (49%)]\tLoss: 0.444287 \tAcc: 0.878075\n",
      "   val:  [11208/21078 (53%)]\tLoss: 0.445273 \tAcc: 0.877231\n",
      "   val:  [12008/21078 (57%)]\tLoss: 0.444231 \tAcc: 0.878165\n",
      "   val:  [12808/21078 (61%)]\tLoss: 0.445222 \tAcc: 0.877967\n",
      "   val:  [13608/21078 (65%)]\tLoss: 0.444013 \tAcc: 0.877572\n",
      "   val:  [14408/21078 (68%)]\tLoss: 0.442926 \tAcc: 0.877568\n",
      "   val:  [15208/21078 (72%)]\tLoss: 0.439063 \tAcc: 0.878156\n",
      "   val:  [16008/21078 (76%)]\tLoss: 0.442484 \tAcc: 0.877811\n",
      "   val:  [16808/21078 (80%)]\tLoss: 0.442335 \tAcc: 0.877499\n",
      "   val:  [17608/21078 (84%)]\tLoss: 0.440475 \tAcc: 0.877499\n",
      "   val:  [18408/21078 (87%)]\tLoss: 0.443323 \tAcc: 0.877336\n",
      "   val:  [19208/21078 (91%)]\tLoss: 0.442263 \tAcc: 0.877655\n",
      "   val:  [20008/21078 (95%)]\tLoss: 0.439705 \tAcc: 0.878349\n",
      "   val:  [20808/21078 (99%)]\tLoss: 0.437205 \tAcc: 0.878364\n",
      "val Loss: 0.4386 Acc: 0.8780\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "Sat Jul  4 08:53:27 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    64W / 280W |   7412MiB / 16160MiB |     36%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "   train:  [16/84314 (0%)]\tLoss: 0.067528 \tAcc: 1.000000\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.382994 \tAcc: 0.884282\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.357312 \tAcc: 0.894590\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.350389 \tAcc: 0.896179\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.351032 \tAcc: 0.898067\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.354618 \tAcc: 0.898328\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.356218 \tAcc: 0.897047\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.356218 \tAcc: 0.897735\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.356338 \tAcc: 0.898330\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.356463 \tAcc: 0.897891\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.358273 \tAcc: 0.897540\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.357600 \tAcc: 0.898104\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.358151 \tAcc: 0.898470\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.357483 \tAcc: 0.898588\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.356289 \tAcc: 0.898911\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.356550 \tAcc: 0.899234\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.358920 \tAcc: 0.898462\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.357972 \tAcc: 0.898883\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.360130 \tAcc: 0.898251\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.359674 \tAcc: 0.897883\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.360251 \tAcc: 0.897614\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.360246 \tAcc: 0.897489\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.360774 \tAcc: 0.897433\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.360745 \tAcc: 0.897599\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.360080 \tAcc: 0.897959\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.359948 \tAcc: 0.898241\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.360578 \tAcc: 0.898116\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.360557 \tAcc: 0.898278\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.361463 \tAcc: 0.898362\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.363212 \tAcc: 0.897794\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.361520 \tAcc: 0.898284\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.361118 \tAcc: 0.898460\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.361587 \tAcc: 0.898567\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.361716 \tAcc: 0.898459\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.361220 \tAcc: 0.898431\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.361506 \tAcc: 0.898297\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.361212 \tAcc: 0.898188\n",
      "   train:  [59216/84314 (70%)]\tLoss: 0.362387 \tAcc: 0.897899\n",
      "   train:  [60816/84314 (72%)]\tLoss: 0.360978 \tAcc: 0.898119\n",
      "   train:  [62416/84314 (74%)]\tLoss: 0.361235 \tAcc: 0.898039\n",
      "   train:  [64016/84314 (76%)]\tLoss: 0.362098 \tAcc: 0.897807\n",
      "   train:  [65616/84314 (78%)]\tLoss: 0.361380 \tAcc: 0.897860\n",
      "   train:  [67216/84314 (80%)]\tLoss: 0.361493 \tAcc: 0.897881\n",
      "   train:  [68816/84314 (82%)]\tLoss: 0.361182 \tAcc: 0.898061\n",
      "   train:  [70416/84314 (84%)]\tLoss: 0.360605 \tAcc: 0.898162\n",
      "   train:  [72016/84314 (85%)]\tLoss: 0.361568 \tAcc: 0.897676\n",
      "   train:  [73616/84314 (87%)]\tLoss: 0.361365 \tAcc: 0.897712\n",
      "   train:  [75216/84314 (89%)]\tLoss: 0.360167 \tAcc: 0.898133\n",
      "   train:  [76816/84314 (91%)]\tLoss: 0.360291 \tAcc: 0.898172\n",
      "   train:  [78416/84314 (93%)]\tLoss: 0.359676 \tAcc: 0.898261\n",
      "   train:  [80016/84314 (95%)]\tLoss: 0.359770 \tAcc: 0.898258\n",
      "   train:  [81616/84314 (97%)]\tLoss: 0.360204 \tAcc: 0.898108\n",
      "   train:  [83216/84314 (99%)]\tLoss: 0.360548 \tAcc: 0.898048\n",
      "train Loss: 0.3604 Acc: 0.8981\n",
      "\n",
      "   val:  [8/21078 (0%)]\tLoss: 0.176187 \tAcc: 1.000000\n",
      "   val:  [808/21078 (4%)]\tLoss: 0.403041 \tAcc: 0.881188\n",
      "   val:  [1608/21078 (8%)]\tLoss: 0.437183 \tAcc: 0.870647\n",
      "   val:  [2408/21078 (11%)]\tLoss: 0.400487 \tAcc: 0.880399\n",
      "   val:  [3208/21078 (15%)]\tLoss: 0.407681 \tAcc: 0.878117\n",
      "   val:  [4008/21078 (19%)]\tLoss: 0.415758 \tAcc: 0.877246\n",
      "   val:  [4808/21078 (23%)]\tLoss: 0.409618 \tAcc: 0.881032\n",
      "   val:  [5608/21078 (27%)]\tLoss: 0.421546 \tAcc: 0.880350\n",
      "   val:  [6408/21078 (30%)]\tLoss: 0.423911 \tAcc: 0.878901\n",
      "   val:  [7208/21078 (34%)]\tLoss: 0.424420 \tAcc: 0.879301\n",
      "   val:  [8008/21078 (38%)]\tLoss: 0.419609 \tAcc: 0.881743\n",
      "   val:  [8808/21078 (42%)]\tLoss: 0.420054 \tAcc: 0.882266\n",
      "   val:  [9608/21078 (46%)]\tLoss: 0.424100 \tAcc: 0.881141\n",
      "   val:  [10408/21078 (49%)]\tLoss: 0.429821 \tAcc: 0.880188\n",
      "   val:  [11208/21078 (53%)]\tLoss: 0.431108 \tAcc: 0.880264\n",
      "   val:  [12008/21078 (57%)]\tLoss: 0.432851 \tAcc: 0.880247\n",
      "   val:  [12808/21078 (61%)]\tLoss: 0.430395 \tAcc: 0.880621\n",
      "   val:  [13608/21078 (65%)]\tLoss: 0.431500 \tAcc: 0.880585\n",
      "   val:  [14408/21078 (68%)]\tLoss: 0.433857 \tAcc: 0.880483\n",
      "   val:  [15208/21078 (72%)]\tLoss: 0.434914 \tAcc: 0.880392\n",
      "   val:  [16008/21078 (76%)]\tLoss: 0.434056 \tAcc: 0.880310\n",
      "   val:  [16808/21078 (80%)]\tLoss: 0.431816 \tAcc: 0.880652\n",
      "   val:  [17608/21078 (84%)]\tLoss: 0.433443 \tAcc: 0.880111\n",
      "   val:  [18408/21078 (87%)]\tLoss: 0.430534 \tAcc: 0.880704\n",
      "   val:  [19208/21078 (91%)]\tLoss: 0.431509 \tAcc: 0.880571\n",
      "   val:  [20008/21078 (95%)]\tLoss: 0.430949 \tAcc: 0.880598\n",
      "   val:  [20808/21078 (99%)]\tLoss: 0.432867 \tAcc: 0.879950\n",
      "val Loss: 0.4335 Acc: 0.8795\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "Sat Jul  4 09:54:36 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    63W / 280W |   7090MiB / 16160MiB |     47%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "   train:  [16/84314 (0%)]\tLoss: 0.705705 \tAcc: 0.812500\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.351438 \tAcc: 0.905941\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.328978 \tAcc: 0.907649\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.340912 \tAcc: 0.902616\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.344925 \tAcc: 0.900249\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.351637 \tAcc: 0.898703\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.346859 \tAcc: 0.899854\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.346231 \tAcc: 0.900321\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.344468 \tAcc: 0.900983\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.344425 \tAcc: 0.901151\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.347120 \tAcc: 0.900287\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.345896 \tAcc: 0.900545\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.346810 \tAcc: 0.900812\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.347582 \tAcc: 0.900557\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.347280 \tAcc: 0.900651\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.346592 \tAcc: 0.900483\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.344522 \tAcc: 0.900960\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.346725 \tAcc: 0.900867\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.346720 \tAcc: 0.901062\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.345278 \tAcc: 0.900907\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.344862 \tAcc: 0.900987\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.345222 \tAcc: 0.901148\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.345337 \tAcc: 0.901352\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.345638 \tAcc: 0.901184\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.347261 \tAcc: 0.900536\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.348294 \tAcc: 0.900090\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.348012 \tAcc: 0.899918\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.347951 \tAcc: 0.900176\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.348588 \tAcc: 0.900147\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.348856 \tAcc: 0.900142\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.349074 \tAcc: 0.900221\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.348018 \tAcc: 0.900536\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.349142 \tAcc: 0.900266\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.348948 \tAcc: 0.900106\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.348502 \tAcc: 0.900470\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.348700 \tAcc: 0.900386\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.349312 \tAcc: 0.900028\n",
      "   train:  [59216/84314 (70%)]\tLoss: 0.349462 \tAcc: 0.899824\n",
      "   train:  [60816/84314 (72%)]\tLoss: 0.348586 \tAcc: 0.900026\n",
      "   train:  [62416/84314 (74%)]\tLoss: 0.349294 \tAcc: 0.900010\n",
      "   train:  [64016/84314 (76%)]\tLoss: 0.348880 \tAcc: 0.900072\n",
      "   train:  [65616/84314 (78%)]\tLoss: 0.348133 \tAcc: 0.900314\n",
      "   train:  [67216/84314 (80%)]\tLoss: 0.348331 \tAcc: 0.900306\n",
      "   train:  [68816/84314 (82%)]\tLoss: 0.349262 \tAcc: 0.900110\n",
      "   train:  [70416/84314 (84%)]\tLoss: 0.350721 \tAcc: 0.899796\n",
      "   train:  [72016/84314 (85%)]\tLoss: 0.349736 \tAcc: 0.899911\n",
      "   train:  [73616/84314 (87%)]\tLoss: 0.350142 \tAcc: 0.899927\n",
      "   train:  [75216/84314 (89%)]\tLoss: 0.349800 \tAcc: 0.899942\n",
      "   train:  [76816/84314 (91%)]\tLoss: 0.350411 \tAcc: 0.899773\n",
      "   train:  [78416/84314 (93%)]\tLoss: 0.351084 \tAcc: 0.899625\n",
      "   train:  [80016/84314 (95%)]\tLoss: 0.351564 \tAcc: 0.899458\n",
      "   train:  [81616/84314 (97%)]\tLoss: 0.352014 \tAcc: 0.899395\n",
      "   train:  [83216/84314 (99%)]\tLoss: 0.351616 \tAcc: 0.899575\n",
      "train Loss: 0.3521 Acc: 0.8994\n",
      "\n",
      "   val:  [8/21078 (0%)]\tLoss: 0.355761 \tAcc: 0.875000\n",
      "   val:  [808/21078 (4%)]\tLoss: 0.437798 \tAcc: 0.879950\n",
      "   val:  [1608/21078 (8%)]\tLoss: 0.437240 \tAcc: 0.875622\n",
      "   val:  [2408/21078 (11%)]\tLoss: 0.427966 \tAcc: 0.877907\n",
      "   val:  [3208/21078 (15%)]\tLoss: 0.434418 \tAcc: 0.875935\n",
      "   val:  [4008/21078 (19%)]\tLoss: 0.432794 \tAcc: 0.876996\n",
      "   val:  [4808/21078 (23%)]\tLoss: 0.435713 \tAcc: 0.877704\n",
      "   val:  [5608/21078 (27%)]\tLoss: 0.451049 \tAcc: 0.875178\n",
      "   val:  [6408/21078 (30%)]\tLoss: 0.445871 \tAcc: 0.877029\n",
      "   val:  [7208/21078 (34%)]\tLoss: 0.453846 \tAcc: 0.874861\n",
      "   val:  [8008/21078 (38%)]\tLoss: 0.440066 \tAcc: 0.878122\n",
      "   val:  [8808/21078 (42%)]\tLoss: 0.443489 \tAcc: 0.877725\n",
      "   val:  [9608/21078 (46%)]\tLoss: 0.443778 \tAcc: 0.878122\n",
      "   val:  [10408/21078 (49%)]\tLoss: 0.446052 \tAcc: 0.877594\n",
      "   val:  [11208/21078 (53%)]\tLoss: 0.448559 \tAcc: 0.876874\n",
      "   val:  [12008/21078 (57%)]\tLoss: 0.443687 \tAcc: 0.877498\n",
      "   val:  [12808/21078 (61%)]\tLoss: 0.440602 \tAcc: 0.877108\n",
      "   val:  [13608/21078 (65%)]\tLoss: 0.437806 \tAcc: 0.877425\n",
      "   val:  [14408/21078 (68%)]\tLoss: 0.439626 \tAcc: 0.876249\n",
      "   val:  [15208/21078 (72%)]\tLoss: 0.439596 \tAcc: 0.876315\n",
      "   val:  [16008/21078 (76%)]\tLoss: 0.439358 \tAcc: 0.876062\n",
      "   val:  [16808/21078 (80%)]\tLoss: 0.439648 \tAcc: 0.876011\n",
      "   val:  [17608/21078 (84%)]\tLoss: 0.440476 \tAcc: 0.876022\n",
      "   val:  [18408/21078 (87%)]\tLoss: 0.441588 \tAcc: 0.875054\n",
      "   val:  [19208/21078 (91%)]\tLoss: 0.440523 \tAcc: 0.875573\n",
      "   val:  [20008/21078 (95%)]\tLoss: 0.440112 \tAcc: 0.875900\n",
      "   val:  [20808/21078 (99%)]\tLoss: 0.438354 \tAcc: 0.876490\n",
      "val Loss: 0.4397 Acc: 0.8762\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "Sat Jul  4 11:51:33 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    54W / 280W |   8994MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "   train:  [16/84314 (0%)]\tLoss: 0.527226 \tAcc: 0.812500\n",
      "   train:  [1616/84314 (2%)]\tLoss: 0.286480 \tAcc: 0.916460\n",
      "   train:  [3216/84314 (4%)]\tLoss: 0.329998 \tAcc: 0.906716\n",
      "   train:  [4816/84314 (6%)]\tLoss: 0.345128 \tAcc: 0.902616\n",
      "   train:  [6416/84314 (8%)]\tLoss: 0.353208 \tAcc: 0.898691\n",
      "   train:  [8016/84314 (10%)]\tLoss: 0.353043 \tAcc: 0.898453\n",
      "   train:  [9616/84314 (11%)]\tLoss: 0.350048 \tAcc: 0.898295\n",
      "   train:  [11216/84314 (13%)]\tLoss: 0.353901 \tAcc: 0.898003\n",
      "   train:  [12816/84314 (15%)]\tLoss: 0.351839 \tAcc: 0.899032\n",
      "   train:  [14416/84314 (17%)]\tLoss: 0.355255 \tAcc: 0.898099\n",
      "   train:  [16016/84314 (19%)]\tLoss: 0.353625 \tAcc: 0.898914\n",
      "   train:  [17616/84314 (21%)]\tLoss: 0.352947 \tAcc: 0.898955\n",
      "   train:  [19216/84314 (23%)]\tLoss: 0.353352 \tAcc: 0.898678\n",
      "   train:  [20816/84314 (25%)]\tLoss: 0.351766 \tAcc: 0.900125\n",
      "   train:  [22416/84314 (27%)]\tLoss: 0.351307 \tAcc: 0.899982\n",
      "   train:  [24016/84314 (28%)]\tLoss: 0.350394 \tAcc: 0.900358\n",
      "   train:  [25616/84314 (30%)]\tLoss: 0.349607 \tAcc: 0.900101\n",
      "   train:  [27216/84314 (32%)]\tLoss: 0.352101 \tAcc: 0.899471\n",
      "   train:  [28816/84314 (34%)]\tLoss: 0.350765 \tAcc: 0.899847\n",
      "   train:  [30416/84314 (36%)]\tLoss: 0.349373 \tAcc: 0.900217\n",
      "   train:  [32016/84314 (38%)]\tLoss: 0.348581 \tAcc: 0.900643\n",
      "   train:  [33616/84314 (40%)]\tLoss: 0.347150 \tAcc: 0.900940\n",
      "   train:  [35216/84314 (42%)]\tLoss: 0.346674 \tAcc: 0.901039\n",
      "   train:  [36816/84314 (44%)]\tLoss: 0.346348 \tAcc: 0.901048\n",
      "   train:  [38416/84314 (46%)]\tLoss: 0.345581 \tAcc: 0.901083\n",
      "   train:  [40016/84314 (47%)]\tLoss: 0.346388 \tAcc: 0.900915\n",
      "   train:  [41616/84314 (49%)]\tLoss: 0.347383 \tAcc: 0.900591\n",
      "   train:  [43216/84314 (51%)]\tLoss: 0.346800 \tAcc: 0.900685\n",
      "   train:  [44816/84314 (53%)]\tLoss: 0.346539 \tAcc: 0.901107\n",
      "   train:  [46416/84314 (55%)]\tLoss: 0.347603 \tAcc: 0.900853\n",
      "   train:  [48016/84314 (57%)]\tLoss: 0.346991 \tAcc: 0.901200\n",
      "   train:  [49616/84314 (59%)]\tLoss: 0.346534 \tAcc: 0.901524\n",
      "   train:  [51216/84314 (61%)]\tLoss: 0.346919 \tAcc: 0.901320\n",
      "   train:  [52816/84314 (63%)]\tLoss: 0.347306 \tAcc: 0.901110\n",
      "   train:  [54416/84314 (65%)]\tLoss: 0.348682 \tAcc: 0.900856\n",
      "   train:  [56016/84314 (66%)]\tLoss: 0.347855 \tAcc: 0.900957\n",
      "   train:  [57616/84314 (68%)]\tLoss: 0.347581 \tAcc: 0.900948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5d87b96740db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model_final = train_model(model, criterion, optimizer, exp_lr_scheduler,\n\u001b[1;32m      2\u001b[0m                        \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckp_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'current_checkpoint.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                        best_model_path=model_dir+'best_model.pt')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-143464d170f6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, checkpoint_path, best_model_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_final = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=50, checkpoint_path=ckp_dir+'current_checkpoint.pt',\n",
    "                       best_model_path=model_dir+'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
